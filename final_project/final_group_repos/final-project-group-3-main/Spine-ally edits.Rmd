---
title: "635_group3"
author: "Group3"
date: "4/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)


library(tidyverse)
library(gtsummary)
library(flextable)
library(caret)
library(GGally)
library(ggpubr)
library(e1071)
library(randomForest)
```

Each of us may need to change the working directory each time we run scripts.

Xueyao's WD: "C:/Users/wxycd/Desktop/UNC study/4th Semester/635/Project_final"

Ally's WD: "~/Documents/UNC BIOS MS PROGRAM/BIOS 635/final-project-group-3-main"

```{r setWD and read data}
#feel free to change to your own data folder
setwd("~/Documents/UNC BIOS MS PROGRAM/BIOS 635/final-project-group-3-main")
spine <- read_csv("data/column_3C_weka.csv") %>%
  mutate(class = factor(class))
```

## Data exploration

```{r data exploration}
head(spine)
summary(spine)
sum(is.na(spine)) # No missing data in dataset
```

Boxplot of 6 variables, do we need to clear out the outliers?
```{r boxplot}
plot_list <- list()
continuous_variables <- names(spine)[1:6]
for(i in 1:length(continuous_variables)){
  plot_list[[i]] <- 
    ggplot(data=spine, 
       mapping=aes_string(x="class", y=continuous_variables[i], fill="class"))+
    geom_boxplot(outlier.color = "red")+
    labs(y="Distribution of continuous variables", x="Spine Class")+
    theme_bw()+
    theme(legend.position = "none") +
    annotate("text",  x= 0, y = Inf, label = continuous_variables[i], 
             vjust=1, hjust=0, size = 5)
}
ggarrange(plotlist = plot_list, 
          ncol = 2
          )
```
```{r correlation plot and distribution plot}
library(corrplot)

spine%>%
  select(-class) %>%
  cor() %>%
  corrplot(method="number",
           order = "AOE")


##Scatterplots
ggpairs(spine, 
        ggplot2::aes(colour=class))
```
Summary table
```{r Summary table}
spine %>%
    tbl_summary(
            by=class,
            statistic = list(all_continuous() ~ "{mean} ({sd})",
                             all_categorical() ~ "{n} ({p}%)")
            ) %>%
  add_n() %>%
  add_p(test = list(all_continuous() ~ "aov")) %>%
  as_flex_table() %>%
  bold(part = "header")
```

##Model
Which two or more models should we choose?
```{r model1}
#RANDOM FOREST: We can change values in the code, but this is the stencil that should work for producing errors

individualfolds<- createFolds(y=spine$class, k=5)
per_class_accuracy <- list()

    for(i in 1:length(individualfolds)){
      spine_train <- spine[-individualfolds[[i]],]
      spine_test <- spine[individualfolds[[i]],]
      
      # Try different number of predictors at split
      reg_rf_preds_tune <- list() 
      reg_rf_oob_error <- list()
      tree_num <- c(50, 250, 500)
      # There are 29 predictors
      m_num <- c( (6/2), 6)
      
      counter <- 1
      for(h in 1:length(tree_num)){
        for(j in 1:length(m_num)){
          reg_rf_preds_tune[[counter]] <- randomForest(class~., spine_train,
                                                 ntree=tree_num[h], mtry=m_num[j])
          reg_rf_oob_error[[counter]] <-
            data.frame("tree_size"=tree_num[h],
                       "num_pred" =m_num[j],
                       "oob_error"=reg_rf_preds_tune[[counter]]$err.rate[tree_num[h]])
          counter <- counter+1
        }
      }
      
      reg_rf_oob_error_df <- do.call("rbind", reg_rf_oob_error)
      reg_rf_oob_error_df
      
      # Refit on training using best no. of predictors at split
      best_error <- which(reg_rf_oob_error_df$oob_error==min(reg_rf_oob_error_df$oob_error))
      reg_rf <- randomForest(class~., spine_train,
                             ntree=reg_rf_oob_error_df$tree_size[best_error], mtry=reg_rf_oob_error_df$num_pred[best_error])
      
      # Evaluate on test set
      spine_test$rf_predict <- predict(reg_rf, newdata = spine_test)
     
      #Per-class accuracies
     per_class_accuracy[[i]] <- rep(NA, length(levels(spine_test$class)))
      for(l in 1:length(per_class_accuracy[[i]])){
        per_class_accuracy[[i]][l] <- 
          spine_test %>%
          filter(class==levels(class)[l]) %>%
          summarise(error = 1-sum(rf_predict==levels(class)[l])/n()) %>%
          unlist()
          
        names(per_class_accuracy[[i]])[l] <- 
          paste0("error_", levels(spine_test$class)[l])
      }
  
    }

   data_set_list  <- as.data.frame(t(do.call("rbind", per_class_accuracy) %>%
      apply(MARGIN=2, FUN=mean, na.rm=TRUE)))
   
    data_set_list %>%
      flextable() %>% 
      colformat_num(digits=4) %>%
      add_header_lines(values="CV ERRORS PER CLASS")
    
   data_set_list_se <- as.data.frame(t(do.call("rbind", per_class_accuracy) %>%
      apply(MARGIN=2, FUN=sd, na.rm=TRUE))) 
   
    data_set_list_se %>%
      flextable() %>% 
      colformat_num(digits=4) %>%
      add_header_lines(values="CV ERROR SE PER CLASS")


```