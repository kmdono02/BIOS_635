dim(cancer_data)[2]
# Create grid
total_p <- dim(cancer_data)[2]-1
tuning_grid <- expand.grid(c(50, 250, 500),c(total_p/2, sqrt(total_p), total_p))
tuning_grid
?expand.grid
# Create grid
total_p <- dim(cancer_data)[2]-1
tuning_grid <- expand.grid("trees"=c(50, 250, 500),
"p"=c(total_p/2, sqrt(total_p), total_p))
tuning_grid
?randomForest
rf_tune <- randomForest(TARGET_deathRate~., data=cancer_train,
mtry = tuning_grid$p[j],
ntree = tuning_grid$trees[j])
j=1
tune_results <- list()
rf_tune <- randomForest(TARGET_deathRate~., data=cancer_train,
mtry = tuning_grid$p[j],
ntree = tuning_grid$trees[j])
rf_tune$mse
rf_tune$mse[tuning_grid$trees[j]]
# Tune over grid
tune_results <- c()
for(j in 1:dim(tuning_grid)[1]){
rf_tune <- randomForest(TARGET_deathRate~., data=cancer_train,
mtry = tuning_grid$p[j],
ntree = tuning_grid$trees[j])
tune_results[j] <- rf_tune$mse[tuning_grid$trees[j]]
}
tune_results
cbind(tuning_grid, "mse"=tune_results)
train_tune_results <- cbind(tuning_grid, "mse"=tune_results)
train_tune_results[which(tune_results==min(tune_results)),]
best_tune <- train_tune_results[which(tune_results==min(tune_results)),]
rf_fit <- randomForest(TARGET_deathRate~., data=cancer_train,
mtry = best_tune$p,
ntree = best_tune$trees)
# Test on test data
test_predict <- predict(rf_fit, newdata=cancer_test)
test_results[[i]] <- postResample(test_predict, cancer_test$TARGET_deathRate)
test_results
test_results <- list()
test_results[[i]] <- postResample(test_predict, cancer_test$TARGET_deathRate)
test_results
# Create grid
total_p <- dim(cancer_data)[2]-1
tuning_grid <- expand.grid("trees"=c(50, 250, 500),
"p"=c(total_p/2, sqrt(total_p), total_p))
# Train, tune, test
for(i in 1:length(tt_indices)){
test_results <- list()
# Create train, test sets
cancer_train <- cancer_data[-tt_indices[[i]],]
cancer_test <- cancer_data[tt_indices[[i]],]
# Tune over grid
tune_results <- c()
for(j in 1:dim(tuning_grid)[1]){
rf_tune <- randomForest(TARGET_deathRate~., data=cancer_train,
mtry = tuning_grid$p[j],
ntree = tuning_grid$trees[j])
tune_results[j] <- rf_tune$mse[tuning_grid$trees[j]]
}
train_tune_results <- cbind(tuning_grid, "mse"=tune_results)
best_tune <- train_tune_results[which(tune_results==min(tune_results)),]
# Fit on training use best tune
rf_fit <- randomForest(TARGET_deathRate~., data=cancer_train,
mtry = best_tune$p,
ntree = best_tune$trees)
# Test on test data
test_predict <- predict(rf_fit, newdata=cancer_test)
# Save fold-specific results
test_results[[i]] <- postResample(test_predict, cancer_test$TARGET_deathRate)
}
# Compute CV error estimates and CV SE of estimates
test_results_all <- data.frame(do.call("rbind", test_results)) %>%
mutate(MSE = RMSE^2)
cv_error <- apply(test_results_all, 2, mean)
cv_error_se <- apply(test_results_all, 2, sd)
print("CV error = ")
cv_error
print("CV error SE = ")
cv_error_se
test_results_all
test_results_all
length(tt_indices)
# Create grid
total_p <- dim(cancer_data)[2]-1
tuning_grid <- expand.grid("trees"=c(50, 250, 500),
"p"=c(total_p/2, sqrt(total_p), total_p))
# Train, tune, test
for(i in 1:length(tt_indices)){
test_results <- list()
# Create train, test sets
cancer_train <- cancer_data[-tt_indices[[i]],]
cancer_test <- cancer_data[tt_indices[[i]],]
# Tune over grid
tune_results <- c()
for(j in 1:dim(tuning_grid)[1]){
rf_tune <- randomForest(TARGET_deathRate~., data=cancer_train,
mtry = tuning_grid$p[j],
ntree = tuning_grid$trees[j])
tune_results[j] <- rf_tune$mse[tuning_grid$trees[j]]
}
train_tune_results <- cbind(tuning_grid, "mse"=tune_results)
best_tune <- train_tune_results[which(tune_results==min(tune_results)),]
# Fit on training use best tune
rf_fit <- randomForest(TARGET_deathRate~., data=cancer_train,
mtry = best_tune$p,
ntree = best_tune$trees)
# Test on test data
test_predict <- predict(rf_fit, newdata=cancer_test)
# Save fold-specific results
test_results[[i]] <- postResample(test_predict, cancer_test$TARGET_deathRate)
}
test_results
1:length(tt_indices)
total_p <- dim(cancer_data)[2]-1
tuning_grid <- expand.grid("trees"=c(50, 250, 500),
"p"=c(total_p/2, sqrt(total_p), total_p))
i=1
test_results <- list()
# Create train, test sets
cancer_train <- cancer_data[-tt_indices[[i]],]
cancer_test <- cancer_data[tt_indices[[i]],]
# Tune over grid
tune_results <- c()
for(j in 1:dim(tuning_grid)[1]){
rf_tune <- randomForest(TARGET_deathRate~., data=cancer_train,
mtry = tuning_grid$p[j],
ntree = tuning_grid$trees[j])
tune_results[j] <- rf_tune$mse[tuning_grid$trees[j]]
}
train_tune_results <- cbind(tuning_grid, "mse"=tune_results)
best_tune <- train_tune_results[which(tune_results==min(tune_results)),]
# Fit on training use best tune
rf_fit <- randomForest(TARGET_deathRate~., data=cancer_train,
mtry = best_tune$p,
ntree = best_tune$trees)
rf_fit
# Create grid
total_p <- dim(cancer_data)[2]-1
tuning_grid <- expand.grid("trees"=c(50, 250, 500),
"p"=c(total_p/2, sqrt(total_p), total_p))
test_results <- list()
# Train, tune, test
for(i in 1:length(tt_indices)){
# Create train, test sets
cancer_train <- cancer_data[-tt_indices[[i]],]
cancer_test <- cancer_data[tt_indices[[i]],]
# Tune over grid
tune_results <- c()
for(j in 1:dim(tuning_grid)[1]){
rf_tune <- randomForest(TARGET_deathRate~., data=cancer_train,
mtry = tuning_grid$p[j],
ntree = tuning_grid$trees[j])
tune_results[j] <- rf_tune$mse[tuning_grid$trees[j]]
}
train_tune_results <- cbind(tuning_grid, "mse"=tune_results)
best_tune <- train_tune_results[which(tune_results==min(tune_results)),]
# Fit on training use best tune
rf_fit <- randomForest(TARGET_deathRate~., data=cancer_train,
mtry = best_tune$p,
ntree = best_tune$trees)
# Test on test data
test_predict <- predict(rf_fit, newdata=cancer_test)
# Save fold-specific results
test_results[[i]] <- postResample(test_predict, cancer_test$TARGET_deathRate)
}
# Compute CV error estimates and CV SE of estimates
test_results_all <- data.frame(do.call("rbind", test_results)) %>%
mutate(MSE = RMSE^2)
cv_error <- apply(test_results_all, 2, mean)
cv_error_se <- apply(test_results_all, 2, sd)
print("CV error = ")
cv_error
print("CV error SE = ")
cv_error_se
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
library(tidyverse)
library(caret)
library(rpart)
library(randomForest)
library(gtsummary)
cancer_data <- read_csv("data/cancer_reg.csv") %>%
select(-c("avgAnnCount", "avgDeathsPerYear", "incidenceRate", "Geography")) %>%
drop_na()
# Create folds
fold_k <- 5
tt_indices <- createFolds(y = cancer_data$TARGET_deathRate, k=fold_k)
test_results <- list()
for(i in 1:fold_k){
# Create train, test sets
cancer_train <- cancer_data[-tt_indices[[i]],]
cancer_test <- cancer_data[tt_indices[[i]],]
# Fit CART w/out pruning/tuning
cart_fit <- rpart(TARGET_deathRate~., data=cancer_train)
# Test on test data
test_predict <- predict(cart_fit, newdata=cancer_test)
# Save fold-specific results
test_results[[i]] <- postResample(test_predict, cancer_test$TARGET_deathRate)
}
# Compute CV error estimates and CV SE of estimates
test_results_all_cart <- data.frame(do.call("rbind", test_results)) %>%
mutate(MSE = RMSE^2)
cv_error <- apply(test_results_all_cart, 2, mean)
cv_error_se <- apply(test_results_all_cart, 2, sd)
print("CV error = ")
cv_error
print("CV error SE = ")
cv_error_se
# Use folds from 1B (or can re-create the folds, either is fine)
# Setup function to do tuning methods
train_test_cart <- function(data=cancer_data, folds=tt_indices, method){
set.seed(12)
# Tune
tune_cart <- train(TARGET_deathRate~., data=data, method="rpart", tuneLength=10)
best_params <- tune_cart$bestTune
# Train and test
test_results <- list()
for(i in 1:length(folds)){
data_train <- data[-tt_indices[[i]],]
data_test <- data[tt_indices[[i]],]
# Fit CART with tuning params
if(method=="whole_data"){
cart_fit <- train(TARGET_deathRate~., data=data_train, method="rpart",
tuneGrid = best_params)
}else{
set.seed(12)
cart_fit <- train(TARGET_deathRate~., data=data_train, method="rpart",
tuneLength = 10)
}
# Test on test set
test_predict <- predict(cart_fit, newdata=data_test)
# Save fold-specific results
test_results[[i]] <- postResample(test_predict, data_test[["TARGET_deathRate"]])
}
test_results_all <- data.frame(do.call("rbind", test_results)) %>%
mutate(MSE = RMSE^2)
cv_error <- apply(test_results_all, 2, mean)
cv_error_se <- apply(test_results_all, 2, sd)
results_end <- list("cv_error"=cv_error, "cv_error_se"=cv_error_se)
return(results_end)
}
print("CART: tune using whole dataset")
train_test_cart(method="whole_data")
print("CART: tune using training set only")
train_test_cart(method="train_specific")
# Create grid
total_p <- dim(cancer_data)[2]-1
tuning_grid <- expand.grid("trees"=c(50, 250, 500),
"p"=c(total_p/2, sqrt(total_p), total_p))
test_results <- list()
# Train, tune, test
for(i in 1:length(tt_indices)){
# Create train, test sets
cancer_train <- cancer_data[-tt_indices[[i]],]
cancer_test <- cancer_data[tt_indices[[i]],]
# Tune over grid
tune_results <- c()
for(j in 1:dim(tuning_grid)[1]){
set.seed(12)
rf_tune <- randomForest(TARGET_deathRate~., data=cancer_train,
mtry = tuning_grid$p[j],
ntree = tuning_grid$trees[j])
tune_results[j] <- rf_tune$mse[tuning_grid$trees[j]]
}
train_tune_results <- cbind(tuning_grid, "mse"=tune_results)
best_tune <- train_tune_results[which(tune_results==min(tune_results)),]
# Fit on training use best tune
set.seed(12)
rf_fit <- randomForest(TARGET_deathRate~., data=cancer_train,
mtry = best_tune$p,
ntree = best_tune$trees)
# Test on test data
test_predict <- predict(rf_fit, newdata=cancer_test)
# Save fold-specific results
test_results[[i]] <- postResample(test_predict, cancer_test$TARGET_deathRate)
}
# Compute CV error estimates and CV SE of estimates
test_results_all_rf <- data.frame(do.call("rbind", test_results)) %>%
mutate(MSE = RMSE^2)
cv_error <- apply(test_results_all_rf, 2, mean)
cv_error_se <- apply(test_results_all_rf, 2, sd)
print("CV error = ")
cv_error
print("CV error SE = ")
cv_error_se
# Unpruned CART
print("Unpruned CART")
cv_error <- apply(test_results_all_cart, 2, mean)
cv_error_se <- apply(test_results_all_cart, 2, sd)
print("CV error = ")
cv_error
print("CV error SE = ")
cv_error_se
# CART
print("Pruned CART")
train_test_cart(method="train_specific")
# Random Forest
cv_error <- apply(test_results_all_rf, 2, mean)
cv_error_se <- apply(test_results_all_rf, 2, sd)
print("CV error = ")
cv_error
print("CV error SE = ")
cv_error_se
mses <- c(apply(test_results_all_cart, 2, mean)["MSE"],
train_test_cart(method="train_specific")$cv_error["MSE"],
apply(test_results_all_rf, 2, mean)["MSE"])
ses <- c(apply(test_results_all_cart, 2, sd)["MSE"],
train_test_cart(method="train_specific")$cv_error_se["MSE"],
apply(test_results_all_rf, 2, sd)["MSE"])
mses
ses
all_results <- data.frame(cbind("CV MSE" = mses,
"CV SE" = ses,
"Method" = method))
all_results
mses <- c(apply(test_results_all_cart, 2, mean)["MSE"],
train_test_cart(method="train_specific")$cv_error["MSE"],
apply(test_results_all_rf, 2, mean)["MSE"])
ses <- c(apply(test_results_all_cart, 2, sd)["MSE"],
train_test_cart(method="train_specific")$cv_error_se["MSE"],
apply(test_results_all_rf, 2, sd)["MSE"])
method <- c("CART", "pruned_CART", "RF")
all_results <- data.frame(cbind("CV MSE" = mses,
"CV SE" = ses,
"Method" = method))
all_results
library(flextable)
flextable(all_results)
all_results$CV.MSE
data.frame("CV MSE" = mses,
"CV SE" = ses,
"Method" = method)
all_results <- data.frame("CV MSE" = mses,
"CV SE" = ses,
"Method" = method)
flextable(all_results)
all_results <- data.frame("CV_MSE" = mses,
"CV_SE" = ses,
"Method" = method)
flextable(all_results)
method <- c("CART", "Pruned_CART", "RF")
all_results <- data.frame("CV_MSE" = mses,
"CV_SE" = ses,
"Method" = method)
flextable(all_results)
heart_disease <- read_csv("data/Correct_Dataset.csv")
heart_disease$Target
heart_disease <- read_csv("data/Correct_Dataset.csv") %>%
mutate(Target = factor(ifelse(Target>3, 3, Target))) %>%
select(-X1)
heart_disease$Target
ftable(heart_disease$Target)
# Create grid
total_p <- dim(cancer_data)[2]-1
tuning_grid <- expand.grid("trees"=c(50, 250, 500),
"p"=c(total_p/2, sqrt(total_p), total_p))
test_results <- list()
i=1
total_p <- dim(cancer_data)[2]-1
tuning_grid <- expand.grid("trees"=c(50, 250, 500),
"p"=c(total_p/2, sqrt(total_p), total_p))
test_results <- list()
fold_k <- 5
tt_indices <- createFolds(y = heart_disease$Target, k=fold_k)
i=1
# Create train, test sets
heart_train <- heart_disease[-tt_indices[[i]],]
heart_test <- heart_disease[tt_indices[[i]],]
# Tune over grid
tune_results <- c()
j=1
set.seed(12)
rf_tune <- randomForest(Target~., data=heart_train,
mtry = tuning_grid$p[j],
ntree = tuning_grid$trees[j])
rf_tune$err.rate
rf_tune$err.rate[1, tuning_grid$trees[j]]
tuning_grid$trees[j]
rf_tune$err.rate[1, tuning_grid$trees[j]]
rf_tune$err.rate[tuning_grid$trees[j], 1]
# Tune over grid
tune_results <- c()
for(j in 1:dim(tuning_grid)[1]){
set.seed(12)
rf_tune <- randomForest(Target~., data=heart_train,
mtry = tuning_grid$p[j],
ntree = tuning_grid$trees[j])
tune_results[j] <- rf_tune$err.rate[tuning_grid$trees[j], 1]
}
train_tune_results <- cbind(tuning_grid, "oob_error"=tune_results)
best_tune <- train_tune_results[which(tune_results==min(tune_results)),]
train_tune_results
best_tune
best_tune <- train_tune_results[which(tune_results==min(tune_results)),][1,]
best_tune
set.seed(12)
rf_fit <- randomForest(Target~., data=heart_train,
mtry = best_tune$p,
ntree = best_tune$trees)
# Test on test data
test_predict <- predict(rf_fit, newdata=heart_test)
per_class_accuracy
per_class_accuracy <- rep(NA, length(levels(heart_test$Target)))
per_class_accuracy
# Save fold-specific, class-specific error rates
per_class_accuracy <- rep(NA, length(levels(heart_test$Target)))
for(l in 1:length(per_class_accuracy)){
per_class_accuracy[l] <-
heart_test %>%
filter(Target==levels(Target)[l]) %>%
summarise(accuracy = sum(test_predict==levels(fetal_health_cat)[l])/n()) %>%
unlist()
names(per_class_accuracy)[l] <-
paste0("accuracy_", levels(heart_test$Target)[l])
}
# Test on test data
heart_test$test_predict <- predict(rf_fit, newdata=heart_test)
# Save fold-specific, class-specific error rates
per_class_accuracy <- rep(NA, length(levels(heart_test$Target)))
for(l in 1:length(per_class_accuracy)){
per_class_accuracy[l] <-
heart_test %>%
filter(Target==levels(Target)[l]) %>%
summarise(accuracy = sum(test_predict==levels(fetal_health_cat)[l])/n()) %>%
unlist()
names(per_class_accuracy)[l] <-
paste0("accuracy_", levels(heart_test$Target)[l])
}
# Test on test data
heart_test$test_predict <- predict(rf_fit, newdata=heart_test)
# Save fold-specific, class-specific error rates
per_class_accuracy <- rep(NA, length(levels(heart_test$Target)))
for(l in 1:length(per_class_accuracy)){
per_class_accuracy[l] <-
heart_test %>%
filter(Target==levels(Target)[l]) %>%
summarise(accuracy = sum(test_predict==levels(Target)[l])/n()) %>%
unlist()
names(per_class_accuracy)[l] <-
paste0("accuracy_", levels(heart_test$Target)[l])
}
per_class_accuracy
heart_test$test_predict
# Create grid
total_p <- dim(cancer_data)[2]-1
tuning_grid <- expand.grid("trees"=c(50, 250, 500),
"p"=c(total_p/2, sqrt(total_p), total_p))
test_results <- list()
fold_k <- 5
tt_indices <- createFolds(y = heart_disease$Target, k=fold_k)
# Train, tune, test
for(i in 1:length(tt_indices)){
# Create train, test sets
heart_train <- heart_disease[-tt_indices[[i]],]
heart_test <- heart_disease[tt_indices[[i]],]
# Tune over grid
tune_results <- c()
for(j in 1:dim(tuning_grid)[1]){
set.seed(12)
rf_tune <- randomForest(Target~., data=heart_train,
mtry = tuning_grid$p[j],
ntree = tuning_grid$trees[j])
tune_results[j] <- rf_tune$err.rate[tuning_grid$trees[j], 1]
}
train_tune_results <- cbind(tuning_grid, "oob_error"=tune_results)
best_tune <- train_tune_results[which(tune_results==min(tune_results)),][1,]
# Fit on training use best tune
set.seed(12)
rf_fit <- randomForest(Target~., data=heart_train,
mtry = best_tune$p,
ntree = best_tune$trees)
# Test on test data
heart_test$test_predict <- predict(rf_fit, newdata=heart_test)
# Save fold-specific, class-specific error rates
per_class_accuracy <- rep(NA, length(levels(heart_test$Target)))
for(l in 1:length(per_class_accuracy)){
per_class_accuracy[l] <-
heart_test %>%
filter(Target==levels(Target)[l]) %>%
summarise(accuracy = sum(test_predict==levels(Target)[l])/n()) %>%
unlist()
names(per_class_accuracy)[l] <-
paste0("accuracy_", levels(heart_test$Target)[l])
}
test_results[[i]] <- per_class_accuracy
}
test_results_all_rf <- data.frame(do.call("rbind", test_results))
test_results_all_rf
cv_error <- apply(test_results_all_rf, 2, mean)
cv_error_se <- apply(test_results_all_rf, 2, sd)
print("CV error = ")
cv_error
print("CV error SE = ")
cv_error_se
prop.table(heart_train$Target)
ftable(heart_train$Target)
levels(heart_disease$Target)
data.frame("Class"=levels(heart_disease$Target),
"CV_Accuracy"=cv_error,
"CV_SE"=cv_error_se)
all_results <- data.frame("Class"=levels(heart_disease$Target),
"CV_Accuracy"=cv_error,
"CV_SE"=cv_error_se)
all_results
all_results <- data.frame("Class"=levels(heart_disease$Target),
"CV_Accuracy"=cv_error,
"CV_SE"=cv_error_se)
rownames(all_results) <- NULL
flextable(all_results)
