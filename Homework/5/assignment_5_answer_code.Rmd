---
title: "Homework 5"
subtitle: "BIOS 635"
author: "..."
date: "2/28/2021"
output: html_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, include=TRUE,
                      fig.width = 10, fig.height = 5)
```

```{r packages, echo=TRUE}
library(tidyverse)
library(broom)
library(gtsummary)
library(flextable)
library(gt)
library(caret)
library(GGally)
library(ggpubr)
```

# Introduction
In this assignment you will practice using cross validation with prediction methods you have used in previous assignments.

The data consists of 2126 records of Cardiotocogram exams (CTGs) on 2126 newborn children, with these CTGs a marker of the infant's health health.  A separate dataset which is a subset of these 2126 infants.  A variable in the dataset describes the infant's health based on a doctor's exam.  The goal is to predict this doctor's exam result simply based on the CTG results.

# 1
## A
First, let's read-in the partial data `fetal_health_subset.csv` and keep only variables which are interested in, due to the large number of variables in the dataset.  Only keep the following: `baseline value`, `accelerations`, `fetal_movement`, `uterine_contractions`, `light_decelerations`, `severe_decelerations`, `prolongued_decelerations`, `abnormal_short_term_variability`, `mean_value_of_short_term_variability`, `mean_value_of_long_term_variability`, and `fetal_health`.  

Finally, create a summary statistics table using `tbl_summary` and `flex_table` of the above variables in the dataset, group by `fetal_health`.  Please include:

- Compute means and SDs for the variables
- Include sample sizes for each variable
- Include ANOVA tests for group differences for each variables
- Format the table column headers to be in written, clear English

```{r 1a, echo=TRUE}
set.seed(12)
fetal_health_data <- read.csv("data/fetal_health_subset.csv") %>% 
  select(baseline.value, accelerations, fetal_movement,
         uterine_contractions, light_decelerations, severe_decelerations,
         prolongued_decelerations, abnormal_short_term_variability,
         mean_value_of_short_term_variability, mean_value_of_long_term_variability,
         fetal_health) %>%
 drop_na() %>%
 mutate(fetal_health_cat = 
          factor(fetal_health)) %>%
  select(-fetal_health)


tbl_summary(data= fetal_health_data,
    by=fetal_health_cat,
    statistic= all_continuous() ~ "{mean} ({sd})", 
    include = c('baseline.value', 
         'accelerations', 'fetal_movement', 'uterine_contractions', 
         'light_decelerations', 'severe_decelerations', 
         'prolongued_decelerations', 'abnormal_short_term_variability', 
         'mean_value_of_short_term_variability',
         'mean_value_of_long_term_variability', 'fetal_health_cat'), 
    label = list(baseline.value ~ "Baseline Value",
                 accelerations ~ "Accelerations",
                 fetal_movement ~ "Fetal Movement",
                 uterine_contractions ~ "Uterine Contractions", 
                 light_decelerations ~ "Light Decelerations",
                 severe_decelerations ~ "Severe Decelerations",
                 prolongued_decelerations ~ "Prolongued Decelerations",
                 abnormal_short_term_variability ~ "Abnormal Short Term Variability", 
                 mean_value_of_short_term_variability ~ 
                   "Mean Value Of Short Term Variability",
                 mean_value_of_long_term_variability ~ 
                   "Mean Value Of Long Term Variability",
                 fetal_health_cat ~ "Fetal Health" ), 
    missing='no')%>%
    add_n() %>%
    add_p(test = list(all_continuous() ~ "aov",
                      all_categorical() ~ "chisq.test")) %>%
    as_flex_table() %>%
    bold( bold = TRUE, part = "header")  %>%
    add_header_lines(values="Summary Statistics for Fetal Health Variables of Interest")

```

## B
Now, we will try to predict `fetal_health` with the variable described above, on the `fetal_health_subset.csv` data.  

- First, let's use a 60:40 training:testing data split
- Then, train a K-nearest neighbors (KNN) algorithm on the training dataset and then test on the test set
- Within the training step, choose the tuning parameter based on the maximum accuracy on the training set.  Try 20 different tuning parameters to evaluate (`tuneLength=20`) and print the maximum accuracies for each of these examined parameters
- Using the algorithm trained with the best tuning parameter, test it by predicting the outcome on the test set and report the prediction accuracy **for each of three classes of `fetal_health`**

```{r 1b, echo=TRUE}
set.seed(12)

tt_indicies <- createDataPartition(y=fetal_health_data$fetal_health_cat, p=0.6,
                                     list=FALSE)
  
fetal_health_train <- fetal_health_data[tt_indicies,]
fetal_health_test <- fetal_health_data[-tt_indicies,]
      
knn_train <- train(form=fetal_health_cat~., data=fetal_health_train,
                       method="knn", tuneLength=20)
knn_train

fetal_health_test$pred_knn_fh <- predict(knn_train, newdata=fetal_health_test)

# Per-class accuracies from best tuning parameter-based KNN.  Could also print out per-class sensitivities from confusionMatrix

per_class_accuracy <- rep(NA, length(levels(fetal_health_test$fetal_health_cat)))

# Could also use a list to store accuracies or other object type

for(l in 1:length(levels(fetal_health_test$fetal_health_cat))){
  per_class_accuracy[l] <- 
    fetal_health_test %>%
    filter(fetal_health_cat==levels(fetal_health_cat)[l]) %>%
    summarise(accuracy = sum(pred_knn_fh==levels(fetal_health_cat)[l])/n()) %>%
    unlist()
          
  names(per_class_accuracy)[l] <- 
    paste0("accuracy_", levels(fetal_health_test$fetal_health_cat)[l])
}

per_class_accuracy
```

## C
Now, we do the same as above using K-fold cross validation.  We conduct this process in a few different ways.  First, let's use the entire data to tune and then do cross validation.  Specifically:

- First, choose the best tuning parameter for the KNN algorithm by trying 20 different tuning parameters to evaluate (`tuneLength=20`) and print the accuracies for each of these examined parameters, **in the entire dataset**
- Now, based on this best tuning parameter will conduct K-fold cross validation.  Try CV with the following values of K: 5, 10, and n (leave one out or LOOCV).  For each, when training the KNN on the training folds, **always use the tuning parameter selected in the first step**.  Report the per-class (3 classes for outcome `fetal_health`) CV error and standard error (SE) of this error for all three fold number choices.

```{r 1c, echo=TRUE}
set.seed(12)

data_set_list <- list()
data_set_list_se <- list()
predict_fetal <- list()
individualfolds <- list()
kfolds<-c(5,10,100)

knn_full_data <- train(fetal_health_cat ~., data = fetal_health_data, method = "knn",
                  preProcess = c("center","scale"), tuneLength=20)

for(k in 1:length(kfolds)){
  individualfolds<- createFolds(y=fetal_health_data$fetal_health_cat, k=kfolds[k])
  per_class_accuracy <- list()

    for(i in 1:length(individualfolds)){
      fetal_health_train <- fetal_health_data[-individualfolds[[i]],]
      fetal_health_test <- fetal_health_data[individualfolds[[i]],]
    
      KNNfit<-train(fetal_health_cat ~., data = fetal_health_train, method = "knn",
                  preProcess = c("center","scale"),
                  tuneGrid=knn_full_data$bestTune)
    
      fetal_health_test$predict_fetal <- predict(KNNfit, newdata =
                                                   fetal_health_test,type="raw")
      per_class_accuracy[[i]] <- 
        rep(NA, length(levels(fetal_health_test$fetal_health_cat)))

      for(l in 1:length(per_class_accuracy[[i]])){
        per_class_accuracy[[i]][l] <- 
          fetal_health_test %>%
          filter(fetal_health_cat==levels(fetal_health_cat)[l]) %>%
          summarise(error = 1-sum(predict_fetal==levels(fetal_health_cat)[l])/n()) %>%
          unlist()
          
        names(per_class_accuracy[[i]])[l] <- 
          paste0("error_", levels(fetal_health_test$fetal_health_cat)[l])
      }

    }

   data_set_list[[k]]  <- do.call("rbind", per_class_accuracy) %>%
    apply(MARGIN=2, FUN=mean, na.rm=TRUE)
  
   data_set_list_se[[k]] <- do.call("rbind", per_class_accuracy) %>%
    apply(MARGIN=2, FUN=sd, na.rm=TRUE)
  
   data_set_list[[k]] <- c(data_set_list[[k]], "folds"=kfolds[k])
  
   data_set_list_se[[k]] <- c(data_set_list_se[[k]], "folds"=kfolds[k])
}

data_set_list_all <- do.call("rbind", data_set_list)
data_set_list_se_all <- do.call("rbind", data_set_list_se)

data_set_list_all
data_set_list_se_all
```

## D
Do the same, but this time, choose **training set specific tuning parameter**.  That is, instead of choosing a single tuning parameter value for the entire CV processes, choose the tuning parameter within each training process inside the CV procedure.  Report the same results for the CV process as was done in 1C.  Compare and contrast the CV error and SE in both 1C and 1D.  If differences exist, what could be a plausible reason? (**Hint: recall bias and variance tradeoff**).  Which of the two ways tuning was implemented is correct?

```{r 1d, echo=TRUE}
set.seed(12)

data_set_list <- list()
data_set_list_se <- list()
predict_fetal <- list()
individualfolds <- list()
kfolds<-c(5,10,100)

for(k in 1:length(kfolds)){
  individualfolds<- createFolds(y=fetal_health_data$fetal_health_cat, k=kfolds[k])
  per_class_accuracy <- list()

    for(i in 1:length(individualfolds)){
      fetal_health_train <- fetal_health_data[-individualfolds[[i]],]
      fetal_health_test <- fetal_health_data[individualfolds[[i]],]
    
      KNNfit<-train(fetal_health_cat ~., data = fetal_health_train, method = "knn",
                  preProcess = c("center","scale"),
                  trControl = trainControl(method = "cv", number = 5),
                  tuneLength = 5)
    
      fetal_health_test$predict_fetal <- predict(KNNfit, newdata =
                                                   fetal_health_test,type="raw")
      per_class_accuracy[[i]] <- 
        rep(NA, length(levels(fetal_health_test$fetal_health_cat)))

      for(l in 1:length(per_class_accuracy[[i]])){
        per_class_accuracy[[i]][l] <- 
          fetal_health_test %>%
          filter(fetal_health_cat==levels(fetal_health_cat)[l]) %>%
          summarise(error = 1-sum(predict_fetal==levels(fetal_health_cat)[l])/n()) %>%
          unlist()
          
        names(per_class_accuracy[[i]])[l] <- 
          paste0("error_", levels(fetal_health_test$fetal_health_cat)[l])
      }

    }

   data_set_list[[k]]  <- do.call("rbind", per_class_accuracy) %>%
    apply(MARGIN=2, FUN=mean, na.rm=TRUE)
  
   data_set_list_se[[k]] <- do.call("rbind", per_class_accuracy) %>%
    apply(MARGIN=2, FUN=sd, na.rm=TRUE)
  
   data_set_list[[k]] <- c(data_set_list[[k]], "folds"=kfolds[k])
  
   data_set_list_se[[k]] <- c(data_set_list_se[[k]], "folds"=kfolds[k])
}

data_set_list_all <- do.call("rbind", data_set_list)
data_set_list_se_all <- do.call("rbind", data_set_list_se)

data_set_list_all
data_set_list_se_all
```

Compare the results between 1C and 1D, we don't see very noticeable differences between the testing results.  However, the tuning process done in 1C is incorrect and biased, as it **was not** incorporated into the cross validation procedure and instead was done separately on the entire dataset.  Thus, since the tuning parameter effects the trained KNN algorithm, this creates an overlap between those in the training and those in the testing sets, creating biased estimates for the estimated test set performance.  **Extra**:  The lack of a noticeable is likely due to the use of a simple algorithm like KNN, which has a low propensity to overfit to the data and has a very simple tuning parameter which does not have a large range of plausible values.  We will see later in the course that the use of tuning in this way can result in **highly biased** performance estimates for more complex methods.

We also see that while the accuracies are similar as we change the number of folds from 5 to LOOCV (or 200/100), the estimated accuracies are fairly stable but the standard errors (SEs) increase dramatically.  This increase in SEs is expected due to the bias-variance tradeoff associated with the choice of fold number.

## E
Finally, we will look at the variability in the 60:40 training:testing performance and compare this to the variability of K-fold CV.  We will use 5-fold CV for this process.

- First, repeat the analysis done in 1B 100 times, each with different 60:40 splits (simply change the seed to get reproducible splits, see lecture slides).  For each of these 100 times, save the per-class (3 classes  for outcome `fetal_health`) prediction accuracy.  
- Secondly, repeat the analysis done in 1D 100 times, with 5-fold CV only.  Save the per-class (3 classes) CV error for all three fold number choices.
- Finally, construct boxplots for these 100 error rates from the 60:40 split procedure, boxplots for the 100 error rates from the 5-fold CV procedure, as well as report in a table the mean and variance in error rates for both the 60:40 procedure and 5-fold CV procedure, for each of the three classes.  Based on this table, do you notice any difference between these per-class error rate distributions?  What could be the cause?

```{r 1e, echo=TRUE}
set.seed(12)

data_set_list_cv <- list()
data_set_list_all <- list()
predict_fetal <- list()
individualfolds <- list()
kfolds<-5
trials <- 100

# Outer loop: number of trials
for(t in 1:trials){
  
  ################### First, do 60:40 split analysis #########################
  tt_indicies <- createDataPartition(y=fetal_health_data$fetal_health_cat, p=0.6,
                                     list=FALSE)
  
  fetal_health_train <- fetal_health_data[tt_indicies,]
  fetal_health_test <- fetal_health_data[-tt_indicies,]
        
  knn_train <- train(form=fetal_health_cat~., data=fetal_health_train,
                         method="knn", 
                     trControl = trainControl(method = "cv", number = 5),
                     tuneLength = 5)
  fetal_health_test$pred_knn_fh <- predict(knn_train, newdata=fetal_health_test)
  
  # Per-class accuracies from best tuning parameter-based KNN
  per_class_accuracy <- rep(NA, length(levels(fetal_health_test$fetal_health_cat)))
  
  for(l in 1:length(levels(fetal_health_test$fetal_health_cat))){
    per_class_accuracy[l] <- 
      fetal_health_test %>%
      filter(fetal_health_cat==levels(fetal_health_cat)[l]) %>%
      summarise(error = 1-sum(pred_knn_fh==levels(fetal_health_cat)[l])/n()) %>%
      unlist()
            
    names(per_class_accuracy)[l] <- 
      paste0("error_", levels(fetal_health_test$fetal_health_cat)[l])
  }
  
  # Add in trial number and method (60:40 split)
  per_class_accuracy_6040_split <- 
    c(per_class_accuracy, "trial"=t, "method"="60_40_split")

  ################### Second, do 5-fold CV #########################
  individualfolds<- createFolds(y=fetal_health_data$fetal_health_cat, k=kfolds)
  per_class_accuracy <- list()
  
    # Do CV analysis for each fold
    for(i in 1:length(individualfolds)){
      fetal_health_train <- fetal_health_data[-individualfolds[[i]],]
      fetal_health_test <- fetal_health_data[individualfolds[[i]],]
      
      KNNfit<-train(fetal_health_cat ~., data = fetal_health_train, method = "knn",
                    preProcess = c("center","scale"),
                    trControl = trainControl(method = "cv", number = 5),
                    tuneLength = 5)
      
      fetal_health_test$predict_fetal <- predict(KNNfit, newdata =
                                                     fetal_health_test,type="raw")
      
      # Save per-class accuracies from CV
      per_class_accuracy[[i]] <- 
          rep(NA, length(levels(fetal_health_test$fetal_health_cat)))
  
      for(l in 1:length(per_class_accuracy[[i]])){
        per_class_accuracy[[i]][l] <- 
          fetal_health_test %>%
          filter(fetal_health_cat==levels(fetal_health_cat)[l]) %>%
          summarise(error = 1-sum(predict_fetal==levels(fetal_health_cat)[l])/n()) %>%
          unlist()
            
        names(per_class_accuracy[[i]])[l] <- 
          paste0("error_", levels(fetal_health_test$fetal_health_cat)[l])
        }
  
      }
  
     # Bind together results from all K folds, take mean for estimate of accuracy
     data_set_list_cv  <- do.call("rbind", per_class_accuracy) %>%
      apply(MARGIN=2, FUN=mean, na.rm=TRUE)
    
     # Add in trial number and method (CV)
     data_set_list_cv <- c(data_set_list_cv, "trial"=t, 
                           "method"=paste0(kfolds, "_fold_CV"))
  
  # Bind goether 60:40 split results and CV results for given trial
  data_set_list_all[[t]] <- 
    data.frame(rbind(per_class_accuracy_6040_split, data_set_list_cv))
}

# Finally, bind together all results from all trials in dataset
data_set_list_all_trials <- do.call("rbind", data_set_list_all)

# Convert error rates to numeric
data_set_list_all_trials[,1:3] <- lapply(data_set_list_all_trials[,1:3],
                                         as.numeric)

# Now create boxplots
plot_list <- list()
error_rates <- c("error_1", "error_2", "error_3")
for(i in 1:length(error_rates)){
  plot_list[[i]] <- 
    ggplot(data=data_set_list_all_trials, 
       mapping=aes_string(x="method", y=error_rates[i], fill="method"))+
    geom_boxplot()+
    labs(y="Estimated test set error", x="Method")+
    theme_bw()+
    theme(legend.position = "none")
}

ggarrange(plotlist = plot_list,
          labels = c("Class 1 Error", "Class 2 Error", "Class 3 Error"),
          nrow=1)

# Also create table
tbl_summary(data=data_set_list_all_trials,
            by=method,
            include=c("error_1", "error_2", "error_3", "method"),
            statistic = list(all_continuous() ~ "{mean} ({sd})"),
            label = list(error_1~"Class 1 Error Rate",
                      error_2~"Class 2 Error Rate",
                      error_3~"Class 3 Error Rate"))
```

From both the table and the boxplots, we see that the 60:40 split and 5-fold CV methods differ in terms of their average test set error and variance of this test set error, on a given data.  Using 100 trials, we see that the variability in the test set performance is much higher in the 60:40 split method then the 5-fold CV method.  This is expected, as 5-fold CV aggregates over multiple test set performance metrics by creating multiple disjoint test sets and then averaging over thier performance.  However, a 60:40 split method only uses a single test set to assess performance, which can be highly variable from one split to the next.  By averaging, 5-fold CV smooths over this split-specific difference.  This is most apparent in the class 2 and 3 performances, as these classes are much less prevalent in the data, increasing the variance in the estimated performance.