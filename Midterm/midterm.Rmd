---
title: "Midterm"
subtitle: "BIOS 635"
author: "..."
date: "3/5/2021"
output: html_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, include=TRUE,
                      fig.width = 10, fig.height = 5)
```

```{r packages, echo=TRUE}
library(tidyverse)
library(broom)
library(gtsummary)
library(flextable)
library(gt)
library(caret)
library(GGally)
library(ggpubr)
library(gam)
library(splines)
library(corrplot)
library(glmnet)
```

# UNC Honor Code

First you must digitally sign the UNC Honor Code by typing in your own below:

UNC Honor Pledge: I certify that no unauthorized assistance has been received or
given in the completion of this work

NAME HERE

You are allowed to use any textual resources from the course (lecture slides, lecture videos, textbooks, etc.).  You can use any outside resources as well, though always put your responses in your own words.  You are not allowed to collaborate with any one else.  You may ask the instructor or teaching assistant any questions and they will answer at their discretion.

# Introduction

In this exam, you will analyze three datasets: one with a continuous outcome to predict, one with a categorical outcome, and one looking at nonlinear relationships.  For all datastes, if there are any observations with missing data, please remove these observations for all analyses.

# 1

The World Health Organization (WHO) gathers data on the wellness of citizens in countries around the world.  In this part, you will analyze WHO data from the years 2000 to 2015 from various countries, containing life expectancy information on the country as well as socioeconomic traits of its citizens.  You will try to see if variables in the dataset can be predict life expectancy.  Information on this dataset can be found at https://www.kaggle.com/kumarajarshi/life-expectancy-who

# A

First, load up the data saved as the CSV file `Life_Expectancy_Data.csv`.  First, just include rows with the years 2000 - 2010.  Then create a summary statistics table grouped by year, but only for data from the years 2000, 2005 and 2010 (for simplicity).

- Include all variables in the data, excluding `Country` and `Status`.  Have `Life expectancy` be the first variable in the table.  Also, bold the variable name from life expectancy and shade the row in gray.
- Include sample size `N` as a column
- For continuous variables, report the means and standard deviations
- For categories variables, report the frequencies and percentages
- Test differences in these variables by year.  Use F tests for continuous variables.  Leave the default testing method for categorical variables
- You can leave the variable labels at their defaults

Second, plot the life expectancy as a function of year **for all of the years in data (2000-2010)**.  Color the points by `Status`, indicating if a country is ``developing'' or otherwise.  Finally, connect the points in a line for each of the countries.

```{r 1a}
# Read in data
who_data <- read_csv("data/Life_Expectancy_Data.csv") %>%
    filter(Year>=2000&Year<=2010) %>%
    drop_na()

# Create summary stats
tbl_summary(data = who_data %>% 
                filter(Year%in%c(2000, 2005, 2010)) %>%
                select(-Country, -Status) %>%
                select(`Life expectancy`, everything()),
            by = Year,
            statistic = list(all_continuous() ~ "{mean} ({sd})", 
                             all_categorical() ~ "{n} ({p}%)")) %>%
    add_n() %>%
    add_p(list(all_continuous() ~ "aov", all_categorical() ~ "chisq.test")) %>%
    as_flex_table() %>%
    bg(i=1, bg="grey") %>%
    bold(part="header")

# Plot over time
ggplot(data=who_data, mapping=aes(x=Year, y=`Life expectancy`, color=Status,
                                  group=Country))+
    geom_point()+
    geom_line()+
    labs(title = "Life expectancy by country for each year from 2000-2010")+
    theme_bw()
```

# B

Now, we will begin to predict life expectancy.  First, let's see how well using older year's life expectancy to predict 2010 expectancy.

- For the data from years 2000 to 2009, train a single linear regression model to predict `Life expectancy` solely based on `Country` as the only predictor.  Then to test, predict the 2010 life expectancy for the countries in the data.  Print out the MSE, MAE, and $R^2$ on the testing set for this model.  **NOTE: remove countries Ireland and Namibia from the 2010 data in this step**
- Are our training and testing sets independent in these cases?  Is there evidence of significant correlation between the life expectancies in 2000 and 2005 with life expectancy in 2010.  Print these correlations (2 of them) in a formatted table include the correlation estimates, p-values, and confidence intervals. **NOTE: Since the same countries won't necessarily be in both the 2000 and 2010 data, you compute the correlations using data from only countries with both 2000 and 2010 life expectancies.  Same goes for the 2000 and 2005 correlations/**
- Is there a training and testing set system that we could use to ensure all of the countries are used in either training and testing (but not both) while still using the years 2000 to 2009 to train and then 2010 to test?  Use a 60:40 split structure to describe such a system.  Then implement this in R and report the MSE and $R^2$ in the test set.  Again you will use linear regression, though the predictors will be 1) `year` and 2) `Status` only.

```{r 1b}
## First bullet
# Create training and test sets
who_train <- who_data %>% 
    filter(Year>=2000&Year<=2009)

who_test <- who_data %>% 
    filter(Year==2010) %>%
    filter(!Country%in%c("Ireland", "Namibia"))

# Create LM model
lm_fit <- lm(`Life expectancy`~Country, data=who_train)

# Test model
who_test$lm_predict <- predict(lm_fit, newdata = who_test)

# Report test set metrics
print("2010 Test Set Metrics: Life Expectancy by Country")
postResample(who_test$lm_predict, who_test$`Life expectancy`)

## Second bullet 
# Compute correlations between 2000 and 2005, along with 2000 and 2010
# Create dataset with columns in 2000, 2005, 2010
data_2000 <- who_data %>% filter(Year==2000) %>% 
    plyr::rename(replace = c("Life expectancy"="LE_2000")) %>%
    select(Country, LE_2000)
data_2005 <- who_data %>% filter(Year==2005) %>% 
    plyr::rename(replace = c("Life expectancy"="LE_2005")) %>%
    select(Country, LE_2005)
data_2010 <- who_data %>% filter(Year==2010) %>% 
    plyr::rename(replace = c("Life expectancy"="LE_2010")) %>%
    select(Country, LE_2010)

data_corrs <- 
    Reduce(function(x,y) full_join(x,y,by="Country"),
           list(data_2000,data_2005,data_2010))

# Compute correlations
corr_list <- list("2000:2010"=cor.test(x=data_corrs$LE_2000, y=data_corrs$LE_2010),
                  "2005:2010"=cor.test(x=data_corrs$LE_2005, y=data_corrs$LE_2010))

# Make function to do it
corr_format <- function(x){
    data.frame("Estimate"=x$estimate,
           "CI_95"=paste0("(", paste0(round(x$conf.int, 2), collapse=", "), ")"),
           "p-value"=ifelse(x$p.value<0.005, "<0.005", round(cor.test(x=x$p.value, 3))))
}

# Use lapply
corr_results <- lapply(corr_list, corr_format)
for(i in 1:length(corr_results)){
    corr_results[[i]]$Year <- names(corr_results)[i]
}

corr_results <- do.call("rbind", corr_results)
rownames(corr_results) <- NULL

## Third bullet 
# First select 60% of countries
who_data_edit <- who_data %>%
    filter(!Country%in%c("Ireland", "Namibia"))

set.seed(12)
training_countries <-
    unique(who_data_edit$Country)[sample(x=1:length(unique(who_data_edit$Country)), 
       size=ceiling(0.6*length(unique(who_data_edit$Country))))]

# Then select remaining
testing_countries <-
    unique(who_data_edit$Country)[-sample(x=1:length(unique(who_data_edit$Country)), 
       size=ceiling(0.6*length(unique(who_data_edit$Country))))]

# Now create training set
who_train <- who_data_edit %>% 
    filter(Year>=2000&Year<=2009&Country%in%training_countries)

who_test <- who_data_edit %>% 
    filter(Year==2010&Country%in%testing_countries)

# Now test and train using year as predictor
# Create LM model
lm_fit <- lm(`Life expectancy`~Year, data=who_train)

# Test model
who_test$lm_predict <- predict(lm_fit, newdata = who_test)

# Report test set metrics
print("2010 Test Set Metrics: Life Expectancy by Year")
postResample(who_test$lm_predict, who_test$`Life expectancy`)
```

- Our training and testing datasets are not independent, as we have countries which are in both the training and testing sets.  We can see that data across years are highly correlated, reinforcing this concern.  
- To address this, we can simply select 60% of the countries to be used as training data in the 2000-2009 data and then the remaining 40% in the 2010 data as the test data.  This means that we **cannot** use country as a predictor (as there is no overlap in this predictor now from training to testing).  When using year, since all data in the 2010 data set has the same year (2010), all values in the test set receieve the same prediction.  To get a more useful prediction model, we would want to use the other predictors in the data with this training and testing structure.
 
# C

Now we will begin to use all of the variables for predicting life expectancy.  **We will select the data in the year 2010 only for this entire part**.  We will consider the following methods

1. K-Nearest Neighbor (KNN)
2. Linear regression
3. Penalized regression

We will do this in the following steps:

1. Create a correlation matrix plot using `corrplot(data, method="number")` from the `corrplot` package, for all of the **numeric** predictors of life expectancy in the data.  By numeric predictors, it means all variables in the data except `Life expectancy`, `Country`, `Status`, and `Year`.
2. Based on these plots, answer the following question:
    a. Are there any noticeably high correlations between these predictors?  How might this influence the results of a linear regression analysis?
3. Print a scatterplot of life expectancy by each **continuous numeric predictor**.  Is there any evidence of a nonlinear relationship between the outcome and the predictors?  If so which ones, and how might you incorporate this nonlinearity in a regression model
4. Again using **only the 2010 data**, train and test 3 prediction models.  1) using KNN, 2) linear regression with **all predictors as linear and all included**, and 3) penalized regression **with all predictors included** (i.e. all variables except `Life expectancy`, `Country`, `Year`)
    a. When training and testing, use 10-fold cross validation
    b. When tuning KNN and penalized regression, incorporate it correctly inside the CV structure
    c. Use Lasso with penalized regression
    d. Print out the CV error for each of the three methods and the standard errors of the CV error
5. Look at the predictors that were removed from the Lasso regression.  What are the coefficient estimates and p-values for these predictors from the linear regression model?  Does it make sense that these predictors were removed based on this information?  **NOTE: You can do this by re-fitting the Lasso and linear regression models to the entire dataset again and looking at the corresponding output (or incorporate the CV results, either is fine)**
6. What are some of the positives and negatives of using a more nonparametric method like KNN compared to linear regression?

```{r 1c, fig.width = 22, fig.height = 18}
## First part
# Filter data
who_data_edit <- who_data %>%
    filter(Year==2010)

# Correlation plot
cors_to_plot <- cor(x=who_data_edit %>% 
                        select(-c("Life expectancy", "Country", "Year", "Status")))
corrplot(cors_to_plot, method="number")

## Third part
scatter_list <- list()
cont_predictors <- names(who_data_edit %>% 
                        select(-c("Life expectancy", "Country", "Year", "Status")))

for(i in 1:length(cont_predictors)){
    scatter_list[[i]] <- 
        ggplot(data=who_data_edit, 
               mapping=aes_string(x=paste0("`", cont_predictors[i], "`"), 
                                  y="`Life expectancy`"))+
            geom_point()+
            geom_smooth(method="lm", se=FALSE)+
            theme_bw()+
            theme(text = element_text(size=20),
                  axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
}

ggarrange(plotlist = scatter_list)

## Fourth part
train_test_fn <-
    function(data=who_data_edit, y_var="Life expectancy",
             x_var=names(who_data_edit %>% 
                             select(-c("Life expectancy", "Country", "Year"))),
             method,
             k_folds=10){
        
        # Create model formula
        model_fmrla <- as.formula(paste0("`", y_var, "` ~", paste0("`", x_var, "`", collapse="+")))
        
        # Create partitions
        tt_indices <- createFolds(y=data[[y_var]], k=k_folds)
        
        # CV train and test
        cv_results <- list()
        for(i in 1:k_folds){
            train_data <- data[-tt_indices[[i]],]
            test_data <- data[tt_indices[[i]],]
            
            # Train
            if(method=="lm"){
                model_fit <- lm(model_fmrla, data=train_data)
            }else{
                if(method=="glmnet"){
                tune_model <- train(model_fmrla, data=train_data,
                                    method=method,
                                    tuneControl=expand.grid(
                                        alpha = 1,
                                        lambda = seq(0.0001, 1, length = 25)))
                model_fit <- tune_model
                
                }else{
                   tune_model <- train(model_fmrla, data=train_data, tuneLength=10,
                                    method=method) 
                   model_fit <- tune_model
                }
            }
            
            # Test
            test_data$predicted <- predict(model_fit, newdata=test_data)
            
            # Report metrics
            cv_results[[i]] <- postResample(test_data$predicted,
                                            test_data[[y_var]])
        }
    
        # Zip everything together to return
        cv_results_all <- do.call("rbind", cv_results)
        return(cv_results_all)
    }

# Run analysis using function
methods <- c("lm", "knn", "glmnet")

analysis_all <- list()
for(i in 1:length(methods)){
    analysis_all[[i]] <- data.frame("cv_est"=apply(train_test_fn(method=methods[i]), 
                                          MARGIN=2, FUN=mean),
                                    "cv_se"=apply(train_test_fn(method=methods[i]), 
                                          MARGIN=2, FUN=sd),
                                    "method"=methods[i])
}

analysis_all_df <- do.call("rbind", analysis_all)
rownames_to_column(analysis_all_df, var="metric") %>% flextable()

## Fifth part
# First lm model on whole dataset
preds <- names(who_data_edit %>% 
                             select(-c("Life expectancy", "Country", "Year")))
lm_check <- lm(as.formula(paste0("`Life expectancy`~", paste0("`", preds, "`", 
                                                              collapse = "+"))), 
               data=who_data_edit)
tidy(lm_check) %>% arrange(abs(estimate))

# Now look at LASSO on whole dataset
lasso_model <- train(as.formula(paste0("`Life expectancy`~", paste0("`", preds, "`", 
                                                              collapse = "+"))),
                    data=who_data_edit,
                    method="glmnet",
                    tuneControl=expand.grid(alpha = 1,
                                        lambda = seq(0.0001, 5, length = 50)))
coef(lasso_model$finalModel, lasso_model$bestTune$lambda)
```

- Based on the correlation heat map, we see noticeably high correlations, including between GDP and expenditure (make sense as GDP is total product produced by country), and diptheria and Hepatits B occurrence (both infectious diseases).  Including these in a regression model together may lead to poorly fitted models due to collinearity, resulting in a loss of precision in our parameter estimates and an inability to accurate identify associations between these affected predictors and the outcome.

- Based on the scatterplots, most of the predictors seem to have linear relationships with the outcome life expectancy.  Some possible exceptions include GDP (seems like it increases sharply then levels off as GDP increases) and Polio/diptheria (seems to sharply increase for high values).  We could incorporate this in a regression model by using nonlinear transformations of these predictors in the model (polynomial, spline, exponential, etc.).

- When looking at the linear regression vs LASSO models on the full data, we see that the LASSO shrunk Hepitatis B and infant deaths to 0 at the "best" lambda parameter.  This is likely due to the high correlations between the thinness variables (infant deaths) and diptheria (Hep B), meaning they are not adding in independent information to the model.

- Some positives about nonparametric methods are 1) they make assumptions (or minimal ones) about the data generating mechanism (mean, distribution, etc.) which reduces the chance of bias from model mispecification.  However, compared to parametric methods, they 1) can overfit to the training data more often, 2) may be more difficult to interpret and 3) may be more computationally expensive (time, power, etc.).


# 2

Now let's consider predicting Alzheimer's diagnosis based on magnetic resonance imaging (MRI) of a person's brain.  150 people are included, between 60 to 96 years old.  While each person has multiple visits and scans, we will only use the first visit's information.  The CDR is a measure of dementia severity with 1 to 2 indicating mild to moderate dementia.  This creates 3 categories: 0=no dementia, 0.5=very mild dementia, 1+=mild to moderate dementia.  You will try to predict this category based on the imaging data and socioeconomic status variables.  See https://www.kaggle.com/jboysen/mri-and-alzheimers more information on the data.

# A

First, load in the data contained in `oasis_longitudinal.csv`.  Then, only keep observations with `visit=1` and re-group `CDR` so that it equals 1+ if `CDR` $\geq 0.5$.  Finally, create a summary statistic table by `CDR`.  Include all of the items mentioned in 1A.  Include all variables except `Subject ID`, `MRI ID`, and `Group`.  These 3 variables can be removed from the dataset.  Treat `SES` as categorical.  Also, compute a panel of scatterplots using `GGally`, colored by `CDR` group.

```{r 2a, fig.width = 22, fig.height = 18}
oasis_data <- read_csv("data/oasis_longitudinal.csv") %>%
    filter(Visit==1) %>%
    mutate(CDR = factor(ifelse(CDR>=0.5, "1+", "0")),
           SES = factor(SES)) %>%
    drop_na()

# Summary stats
tbl_summary(data=oasis_data %>% select(-c("Subject ID", "MRI ID", "Group")),
            by="CDR",
            statistic = list(all_continuous() ~ "{mean} ({sd})", 
                             all_categorical() ~ "{n} ({p}%)")) %>%
    add_n() %>%
    add_p(list(all_continuous() ~ "aov", all_categorical() ~ "chisq.test")) %>%
    as_flex_table()

# Scatterplots
ggpairs(data=oasis_data %>% select(-c("Subject ID", "MRI ID", "Group")),
        ggplot2::aes(colour=CDR))+
    theme(text = element_text(size=20),
          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

# B

For predicting `CDR` we will consider the following methods:

1. KNN
2. Logistic regression
3. LDA/QDA
4. Penalized regression

We will do this in the following steps:

1. Train and test 4 prediction models.  We will first talk about the first 3.  First, based on the results in 2A, determine if LDA or QDA would be more appropriate and explain why you made such a choice.  **NOTE: Any variables which have the same values for all observations (see summary statistics table in 2A should NOT be included as predictors in your analyses for the rest of this part).**
2. Now, train and test the following 3 prediction models: 1) KNN, 2) logistic regression **with all predictors**, and 3) LDA or QDA based on the above, **with all predictors**.  For each, use 10-fold CV and make sure you **choose your tuning parameters within the CV procedure unbiasedly**.  Print the CV error rates and SE error rates for each of the 3 models, by `CDR` class (*by-class error rates and SEs*).  **NOTE: Collapse variable `SES` into 1, 2, 3, 4 where those in category 5 are instead also in category 4.**
3. We will now implement the following version of penalized regression.  First, using Lasso with `CDR` **in it original form as a continuous, numeric variable (0, 0.5, 2)**.  Tune the penalty parameter **based on the entire dataset** by using the default method in `cv.glmnet` and selecting the lambda which minimizes the mean squared error (`lambda.min`).  How is this tuning done by default?  Finally, based on this tuning parameter choice, train and test using 10 fold CV.
4. What is incorrect about the training, tuning, and testing method requested in the part above?  Why is it incorrect and how could you adjust the procedure to be correct?  **You don't need to implement this**
5. What are the potential benefits of using a model selection process such as penalized regression instead of simply using all the included predictors?
6. Of the first three models, which had the lowest **per-class** CV errors?  Are there any noticeable differences between the models' **per-class** SEs?

```{r 2b}
train_test_fn_class <-
    function(data=oasis_data %>% 
                 mutate(SES=factor(ifelse(as.numeric(SES)>4, 4, as.numeric(SES))),
                        `M/F`=factor(`M/F`)), 
             y_var="CDR",
             x_var=names(oasis_data %>% 
                             select(-c("Subject ID", "MRI ID", "Group", "CDR", 
                                       "MR Delay", "Visit", "Hand"))),
             method,
             k_folds=10){
        
        # Create model formula
        model_fmrla <- as.formula(paste0("`", y_var, "` ~", paste0("`", x_var, "`", collapse="+")))
        
        # Create partitions
        tt_indices <- createFolds(y=data[[y_var]], k=k_folds)
        
        # CV train and test
        cv_results <- list()
        for(i in 1:k_folds){
            train_data <- data[-tt_indices[[i]],]
            test_data <- data[tt_indices[[i]],]
            
            # Train
            if(method=="logreg"){
                model_fit <- glm(model_fmrla, data=train_data, family=binomial)
            }
            if(method=="knn"){
                tune_model <- train(model_fmrla, data=train_data,
                                    method=method,
                                    tuneLength=10)
                model_fit <- tune_model
            }
            if(method=="qda"){
                train_data <- train_data %>% 
                    mutate(SES=as.numeric(SES), `M/F`=as.numeric(`M/F`))
                test_data <- test_data %>% 
                    mutate(SES=as.numeric(SES), `M/F`=as.numeric(`M/F`))
                
                model_fit <- train(x=train_data[x_var], y=train_data[[y_var]],
                                    method="qda")
            }
            
            # Test
            test_data$predicted <- 
                if(method=="logreg"){
                    factor(ifelse(predict(model_fit, newdata=test_data, type="response")>0.5,
                           "1+", "0"))
                }else{
                    predict(model_fit, newdata=test_data, type="raw")
                }
            
            # Report metrics
            cv_results[[i]] <- confusionMatrix(test_data$predicted,
                                            test_data[[y_var]], positive = "1+")$byClass
        }
    
        # Zip everything together to return
        cv_results_all <- do.call("rbind", cv_results)
        return(cv_results_all)
    }

# Run analysis using function
methods <- c("logreg", "knn", "qda")

analysis_all <- list()
for(i in 1:length(methods)){
    analysis_all[[i]] <- data.frame("cv_est"=apply(train_test_fn_class(method=methods[i]), 
                                          MARGIN=2, FUN=mean, na.rm=TRUE),
                                    "cv_se"=apply(train_test_fn_class(method=methods[i]), 
                                          MARGIN=2, FUN=sd, na.rm=TRUE),
                                    "method"=methods[i])
}

do.call("rbind", analysis_all)

# LASSO
oasis_data_lasso <- read_csv("data/oasis_longitudinal.csv") %>%
    filter(Visit==1) %>%
    mutate(SES=factor(ifelse(as.numeric(SES)>4, 4, as.numeric(SES))),
           `M/F`=factor(`M/F`)) %>%
    select(-c("Subject ID", "MRI ID", "Group", 
              "MR Delay", "Visit", "Hand")) %>%
    drop_na()

#Tune
tune_lasso <- cv.glmnet(y = oasis_data_lasso$CDR, 
          x = model.matrix(CDR ~ ., data = oasis_data_lasso)[, -1],
          alpha = 1)

#Train and test
# Create partitions
k_folds <- 10
set.seed(12)
tt_indices <- createFolds(y=oasis_data_lasso$CDR, k=k_folds)
        
# CV train and test
cv_results <- list()
for(i in 1:k_folds){
    train_data <- oasis_data_lasso[-tt_indices[[i]],]
    test_data <- oasis_data_lasso[tt_indices[[i]],]
    
    # Use lambda above
    lasso_fit <- 
        glmnet(x=model.matrix(CDR ~ ., data = train_data)[, -1],
           y=train_data$CDR,
           alpha=1,
           lambda = tune_lasso$lambda.min)
    
    # Test
    cv_results[[i]] <-
    postResample(obs=test_data$CDR, 
                 pred=predict(lasso_fit, 
                              newx=model.matrix(CDR ~ ., data = test_data)[, -1]))
}

# Bind results
lasso_cv_results_all <- do.call("rbind", cv_results)

# Print CV error
print("Lasso CV error:")
apply(lasso_cv_results_all, 2, mean)
```
- QDA was chosen as LDA assumes that each class has the same predictor covariance matrix.  We can see that this is not the case by looking at the correlations reported in the above plot by class (ex. EDUC and eTIV, DUC and ASF).
- The results for logistic regression and QDA are very similar in terms sensitivity and specificity, with KNN being noticeably worse and QDA having noticeably less SE in both measures
- The tuning process used above is incorrect (biased) as we used all of the data to tune and then ran training and testing through CV, we can result in overfitting and result in biased testing results.  We can correct this by tuning each time we do training in the CV process.  By default, if using `cv.glmnet`, a grid search is done where each parameter set in the grid is used to train a penalized regression model, with the training and testing done using 10-fold CV.  If `train` is used, the bootstrap with 25 resamples is used to do this same process.
- Using a model selection approach can reduce the number of features used in the prediction model, thus reducing the complexity of the model and the propensity to overfit (lower the variance in the bias-variance tradeoff framework).

# C

Recall we removed all visits except the first one in 2A.  Consider using all of the visits to predict `CDR` as a binary variable **(0 or 1+ as defined in 2A)**.  You will not do the analysis but will discuss about some proposed analysis ideas.  Critique these ideas and identify why they would be incorrect.  For the training and testing data partitioning method suggested, explain how you would fix this method.

1. Use logistic regression or LDA/QDA on data which includes multiple visits for some subjects.  We will train and test on a 60:40 split of such a dataset.
2. Do the same, but train and test using 10-fold CV on the whole dataset.

- In order to do a 60:40 split and maintain unbiased training and testing sets, we much make sure we have completely independent sets.  Thus, due to the multiple observations per person we have clustered data (each person is a cluster).  Thus, we can randomly select 60% of the people including all of their visits, as the training set, with the remaining as the test set.  Note that there is still the lack of independence in a given subject, which violates the assumptions of independent training set data for logsitic regression and LDA/QDA (need a longitudinal model; beyond the scope of this course).
- We again need to account for the clustering, this time in the CV process.  We can simply partition the set of IDs into 10 equal pieces, and have 9 out of 10 of these be the training set and the other 1 set be the testing set.  We repeat this for each of the 10 partitions.

# 3

To look at non-linearity, you work with a dataset of individuals who applied for a banking loan.  Each row of the dataset contains an individual applying for a loan, with various characteristics about them, information on the overall area's economy, as well as if they received the loan ("yes" or "no").  Your goal is to train and test an algorithm to predict if a person receives a loan based on these characteristics.  See https://www.kaggle.com/henriqueyamahata/bank-marketing for information on the dataset (as well the txt file `bank-additional-names.txt`).

# A

First, load up the data saved as the CSV file `bank_data_subset.csv`.  Only read in the following variables: `age`, `job`, `marital`, `education`, `default`, `housing`, `loan`, `emp.var.rate`, `cons.price.idx`, `cons.conf.idx`, `nr.employed`, and `y`.  Then create a summary statistics table grouped by outcome (received loan) variable `y`.  

- Include all variables in the data.  Bold all column headers.  Group statistics by `y`.
- Include sample size `N` as a column
- For continuous variables, report the means and standard deviations
- For categories variables, report the frequencies and percentages
- Test differences in these variables by year.  Use F tests for continuous variables.  Leave the default testing method for categorical variables
- You can leave the variable labels at their defaults

```{r 3a}
bank_data <- read_csv("data/bank_data_subset.csv") %>%
    select(age, job, marital, education, default, housing, loan, `emp.var.rate`, 
           `cons.price.idx`, `cons.conf.idx`, `nr.employed`, y) %>%
    mutate(y=factor(y)) %>%
    drop_na()

tbl_summary(data=bank_data, 
            by=y,
            statistic = list(all_continuous() ~ "{mean} ({sd})", 
                             all_categorical() ~ "{n} ({p}%)")) %>%
    add_n() %>%
    add_p(list(all_continuous() ~ "aov", all_categorical() ~ "chisq.test")) %>%
    as_flex_table() %>%
    bold(part="header")
```

# B

Now, create a panel of scatterplots for these variables, colored by `y` using GGally.  Interpret any noticeably non-zero correlations, and denote there are any signs of collinearity between these variables.  Does it make sense to use correlations of the categorical variables?  If so why?  If not, why not?

```{r 3b, fig.width = 22, fig.height = 18}
ggpairs(bank_data, mapping=aes(color=y))+
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

For the continuous variables, we see high correlations between the economic wellness variables, specifically employment variance rate and the consumer price index, as well as the number employed.  This makes sense, given how related different indices of the economy are (production, growth, employment, etc.).  Correlations between categorical variables don't make much sense, as they imply a linear relationship between the variables.  This would mean a linear relationship between the two sets of categories, which would be informative if the categories represent ordinal variables (increasing and decreasing with inherent order).

# C

Finally, we will train and test a logistic regression algorithm allowing for nonlinearity using generalized additive models (GAM).  Use the `gam` package to fit these models (and make sure `y` is a factor variable!),  You will consider 2 models.  Both will have the following categorical predictors: `job`, `marital`, `education`, `default`, `housing`, `loan` and `y` as the outcome.  The models are differentiated by how the following predictors are used: `age`, `emp.var.rate`, `cons.price.idx`, and `cons.conf.idx`.  The other variables in the data are omitted from all models.  **NOTE: Any variables with the same values for all observations should be omitted from all models as well.**

1. `age`, `emp.var.rate`, `cons.price.idx`, and `cons.conf.idx` as linear
2. `age`, `cons.price.idx`, and `cons.conf.idx` as natural cubic splines (**leave `emp.var.rate` as linear**).  Choose the knots using `df=4`.

Use 5-fold CV to train and test your algorithms.  Report either 1) the CV prediction accuracy and CV standard error (SE) of this accuracy **or** 2) the CV prediction error and CV standard error (SE) of this error for each of these two models in a labeled `flextable`, based on a 0.5 threshold.  Did the addition of the nonlinear spline terms improve the model fit?  Also, **what quantiles of the predictors were the knots place?**  (Hint: using `ns(bank_data$age, df=4)` will tell you)

```{r 3c}
train_test_fn <-
    function(data=bank_data, y_var="y",
             x_var=names(bank_data %>% 
                             select(-c("y"))),
             method,
             k_folds=10){
        
        # Create partitions
        tt_indices <- createFolds(y=data[[y_var]], k=k_folds)
        
        # CV train and test
        cv_results <- list()
        for(i in 1:k_folds){
            train_data <- data[-tt_indices[[i]],]
            test_data <- data[tt_indices[[i]],]
            
            # Train
            if(method=="linear"){
                # Create model formula
                model_fmrla <- as.formula(paste0("`", y_var, "` ~", 
                                         paste0("`", x_var, "`", collapse="+")))
        
                model_fit <- gam(formula=model_fmrla, 
                                 family=binomial,
                                 data=train_data)
            }else{
                if(method=="spline"){
                    cubic_preds <- c("age", "cons.price.idx", "cons.conf.idx")
                    linear_preds <- x_var[!x_var%in%cubic_preds]
                    model_fmrla <- as.formula(paste0("`", y_var, "` ~", 
                                             paste0(linear_preds, collapse = " + "), "+",
                                             paste0("ns(`", cubic_preds, "`, df=4)", 
                                                    collapse="+")))
                    
                    model_fit <- gam(formula=model_fmrla, 
                                     family=binomial,
                                     data=train_data)
                }else{
                   stop("No applicable model selected")
                }
            }
            
            # Test
            test_data$predicted <- 
                factor(ifelse(predict(model_fit, newdata=test_data, type="response")>0.5, 
                       "yes", "no"))
            
            # Report metrics
            cv_results[[i]] <- postResample(test_data$predicted,
                                            test_data[[y_var]])
        }
    
        # Zip everything together to return
        cv_results_all <- do.call("rbind", cv_results)
        return(cv_results_all)
    }

# Do for linear and spline
methods <- c("linear", "spline")

analysis_all <- list()
for(i in 1:length(methods)){
    analysis_all[[i]] <- data.frame("cv_est"=apply(train_test_fn(method=methods[i]), 
                                          MARGIN=2, FUN=mean),
                                    "cv_se"=apply(train_test_fn(method=methods[i]), 
                                          MARGIN=2, FUN=sd),
                                    "method"=methods[i])
}

do.call("rbind", analysis_all)

attr(ns(bank_data$age, df=4), "knots")
```

- The knots are placed at the 0.25, 0.5, and 0.75 quantiles.
- We have essentially the same prediction accuracy estimates for both the linear and spline models, but much higher SEs for the spline model (as expected due to the more complex model).  Thus, a linear prediction models seems to suffice (though we may want to check the per-class accuracy rates to more robustly compare the models).